"use strict";(self.webpackChunkstable_wiki=self.webpackChunkstable_wiki||[]).push([[335],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>f});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=i.createContext({}),c=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return i.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(n),m=a,f=d["".concat(s,".").concat(m)]||d[m]||u[m]||r;return n?i.createElement(f,o(o({ref:t},p),{},{components:n})):i.createElement(f,o({ref:t},p))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:a,o[1]=l;for(var c=2;c<r;c++)o[c]=n[c];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}m.displayName="MDXCreateElement"},1220:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=n(7462),a=(n(7294),n(3905));const r={sidebar_position:2},o="Image Variation",l={unversionedId:"features/image-variation",id:"features/image-variation",title:"Image Variation",description:"Technology",source:"@site/dynamics/features/image-variation.md",sourceDirName:"features",slug:"/features/image-variation",permalink:"/dynamics/features/image-variation",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"parameters",previous:{title:"Depth",permalink:"/dynamics/features/depth"}},s={},c=[{value:"Technology",id:"technology",level:2},{value:"Stable unCLIP",id:"stable-unclip",level:3},{value:"Demo",id:"demo",level:2},{value:"Code",id:"code",level:2}],p={toc:c},d="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(d,(0,i.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"image-variation"},"Image Variation"),(0,a.kt)("h2",{id:"technology"},"Technology"),(0,a.kt)("p",null,"The classic text-to-image Stable Diffusion model is trained to be conditioned on text inputs."),(0,a.kt)("p",null,"This version replaces the original text encoder with an image encoder. Instead of generating images based on text input, images are generated from an image. Some noise is added to generate variation after the encoder is put through the algorithm."),(0,a.kt)("p",null,"This approach produces similar looking images with different details and compositions. Unlike the ",(0,a.kt)("a",{parentName:"p",href:"/dynamics/base-features/image-to-image"},"image-to-image")," algorithm, the source image is first fully encoded. This means the generator does not use a single pixel sourced from the original image(@islamovic_2023)."),(0,a.kt)("h3",{id:"stable-unclip"},"Stable unCLIP"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://openai.com/dall-e-2/"},"unCLIP")," is the approach behind OpenAI's ",(0,a.kt)("a",{parentName:"p",href:"https://openai.com/dall-e-2/"},"DALL\xb7E 2"),",\ntrained to invert CLIP image embeddings.\nWe finetuned SD 2.1 to accept a CLIP ViT-L/14 image embedding in addition to the text encodings.\nThis means that the model can be used to produce image variations, but can also be combined with a text-to-image\nembedding prior to yield a full text-to-image model at 768x768 resolution."),(0,a.kt)("p",null,"We provide two models, trained on OpenAI CLIP-L and OpenCLIP-H image embeddings, respectively,\navailable from ",(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/tree/main"},"https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip"),".\nTo use them, download from Hugging Face, and put and the weights into the ",(0,a.kt)("inlineCode",{parentName:"p"},"checkpoints")," folder."),(0,a.kt)("h1",{id:"usage"},"Usage"),(0,a.kt)("h2",{id:"demo"},"Demo"),(0,a.kt)("p",null,"If you would like to try a demo of this model on the web, please visit ",(0,a.kt)("a",{parentName:"p",href:"https://clipdrop.co/stable-diffusion-reimagine"},"https://clipdrop.co/stable-diffusion-reimagine")),(0,a.kt)("h2",{id:"code"},"Code"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://github.com/Stability-AI/stablediffusion/blob/main/doc/UNCLIP.MD"},"How to use with diffusers library")))}u.isMDXComponent=!0}}]);