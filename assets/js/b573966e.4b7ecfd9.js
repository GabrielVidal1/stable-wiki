"use strict";(self.webpackChunkstable_wiki=self.webpackChunkstable_wiki||[]).push([[41],{93776:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>A,contentTitle:()=>N,default:()=>H,frontMatter:()=>S,metadata:()=>L,toc:()=>I});var i=a(87462),s=a(67294),n=a(3905),o=a(11570);const r=e=>{let{name:t,children:a}=e;return s.createElement("div",{className:"flex flex-row items-center align-baseline relative px-4"},s.createElement("p",{className:"w-1/2 m-0"},t),s.createElement(o.HH,{id:t+"-source",type:"source",position:o.Ly.Right,className:"w-16  !bg-teal-500 absolute"}),s.createElement(o.HH,{id:t+"-target",type:"target",position:o.Ly.Left,className:"w-16 !bg-teal-500"}),a)},l=e=>{let{name:t,options:a,...n}=e;return s.createElement(r,{name:t},s.createElement("select",(0,i.Z)({},n,{className:"w-1/2 p-1 border border-gray-300 rounded-md"}),a.map((e=>s.createElement("option",{key:e,value:e,className:"text-ellipsis"},e)))))},d=e=>{let{name:t,children:a,styles:i}=e;return s.createElement("div",{className:"py-2 bg-white justify-center shadow-lg rounded-xl flex flex-col gap-2",style:{width:"200px",...i}},s.createElement("h3",{className:"m-0 px-4 text-center"},t),a)},p=e=>{let{data:{name:t,datatype:a,props:n,prompt:o}}=e;return s.createElement(d,{name:"Data"},s.createElement(l,(0,i.Z)({},n,{name:"Select"})),s.createElement("div",{className:"text-sm mx-2 p-2 border-2 border-gray-200 border-solid"},o))},m=(0,s.memo)(p);var u=a(83599);a(99869);const c=e=>{let{emoji:t,job:a,name:i}=e;return s.createElement(d,{name:i},s.createElement(o.HH,{type:"target",position:o.Ly.Left,className:"w-16 !bg-teal-500"}),s.createElement(o.HH,{type:"source",position:o.Ly.Right,className:"w-16 !bg-teal-500"}))},f=e=>{let{data:{img:t,name:a}}=e;return s.createElement(d,{name:"Image"},s.createElement(o.HH,{type:"target",position:o.Ly.Left,className:"w-16 !bg-teal-500"}),s.createElement("div",{className:"aspect-square mx-4 h-fit"},s.createElement("img",{src:t,className:"rounded"})))},h=e=>{let{}=e;return s.createElement(d,{name:"Stable Diffusion"},s.createElement("img",{src:a(13026).Z}),s.createElement(o.HH,{type:"target",position:o.Ly.Left,className:"w-16 !bg-teal-500"}),s.createElement(o.HH,{type:"source",position:o.Ly.Right,className:"w-16 !bg-teal-500"}))},b={textEncoder:(0,s.memo)(c),imageOut:(0,s.memo)(f),stableDiffusion:(0,s.memo)(h)},g={animated:!0,style:{stroke:"white",animated:!1}},y={stroke:"white",strokeWidth:2},w=e=>{let{nodeTypes:t,...a}=e;const{setViewport:n,zoomIn:r,zoomOut:l}=(0,o._K)(),d=((0,o.Sj)(),(0,s.useCallback)((()=>{n(a.defaultViewport,{duration:800})}),[n]));return s.createElement("div",{style:{height:"400px"}},s.createElement(o.x$,(0,i.Z)({},a,{style:{backgroundColor:"#D3D2E5"},defaultEdgeOptions:g,nodeTypes:{...b,...t},connectionLineStyle:y}),s.createElement(u.A,null),s.createElement("div",{className:"controls flex gap-2"},s.createElement("button",{onClick:d},"Reset View"))))};const k=function(e){return s.createElement(o.tV,null,s.createElement(w,e))},v=[{prompt:"A picture of a cat",image:a.p+"assets/images/cat-de78e4ddbbd4617f6d0f100a380a4304.jpeg"},{prompt:"A picture of a dog",image:a.p+"assets/images/dog-0d6c5609f864c4d4926e3e3ab1025ba5.webp"},{prompt:"fish eyed view of a capybara, photorealistic",image:a.p+"assets/images/fisheyed-capybara-25bfc0beba70c4516da70b6cb49a7025.jpeg"}],E={x:-30,y:103,zoom:.88},x=()=>{const[e,t]=s.useState(v[0]),a=(0,s.useMemo)((()=>({prompt:m})),[]),i=(0,s.useMemo)((()=>[{id:"prompt",source:"prompt",target:"sd",color:"red"},{id:"image",source:"sd",target:"img"}]),[]),n=(0,s.useMemo)((()=>[{id:"prompt",type:"prompt",data:{name:"Prompt",datatype:"text",props:{name:"Prompt",onChange:e=>{const a=v.find((t=>t.prompt===e.target.value));t(a)},value:e.prompt,options:v.map((e=>e.prompt))},prompt:e.prompt},position:{x:100,y:50}},{id:"sd",type:"stableDiffusion",data:{name:"Node A",job:"qsd"},position:{x:400,y:10}},{id:"img",type:"imageOut",data:{img:e.image},position:{x:700,y:0}}]),[e]);return s.createElement(k,{defaultViewport:E,nodeTypes:a,nodes:n,edges:i})},D=(0,s.memo)(x),S={},N="Stable diffusion",L={unversionedId:"stable-diffusion/README",id:"stable-diffusion/README",title:"Stable diffusion",description:"What is Stable Diffusion",source:"@site/docs/stable-diffusion/README.mdx",sourceDirName:"stable-diffusion",slug:"/stable-diffusion/",permalink:"/docs/stable-diffusion/",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Welcome",permalink:"/docs/intro"},next:{title:"VAE",permalink:"/docs/stable-diffusion/VAE"}},A={},I=[{value:"What is Stable Diffusion",id:"what-is-stable-diffusion",level:2},{value:"Origins and Research of Stable Diffusion",id:"origins-and-research-of-stable-diffusion",level:3},{value:"Initial Training Data",id:"initial-training-data",level:4}],T={toc:I},M="wrapper";function H(e){let{components:t,...a}=e;return(0,n.kt)(M,(0,i.Z)({},T,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"stable-diffusion"},"Stable diffusion"),(0,n.kt)(D,{mdxType:"StableDiffusionExample"}),(0,n.kt)("h2",{id:"what-is-stable-diffusion"},"What is Stable Diffusion"),(0,n.kt)("p",null,"Stable Diffusion is an open-source machine learning model that can generate images from text, modify images based on text or enhance low-resolution or low-detail images. It has been trained on billions of images and can produce results that are on par with those generated by DALL-E 2 and MidJourney."),(0,n.kt)("p",null,"Stable Diffusion (SD) is a deep-learning, text-to-image model that was released in 2022. Its primary function is to generate detailed images based on text descriptions. The model uses a combination of random static generation, noise, and pattern recognition through neural nets that are trained on keyword pairs. These pairs correspond to patterns found in a given training image that match a particular keyword."),(0,n.kt)("p",null,"To generate an image, the user inputs a text description, and the SD model references the keyword pairs associated with the words in the description. The model then produces a shape that corresponds to the patterns identified in the image. Over several passes, the image becomes clearer and eventually results in a final image that matches the text prompt."),(0,n.kt)("p",null,"Stable Diffusion is a latent diffusion model, which is a type of deep generative neural network. It was developed by the CompVis group at LMU Munich in collaboration with Stability AI, Runway, EleutherAI, and LAION. In October 2022, Stability AI raised US$101 million in a round led by Lightspeed Venture Partners and Coatue Management."),(0,n.kt)("p",null,"Stable Diffusion's code and model weights have been released publicly, and it can run on most consumer hardware equipped with a modest GPU with at least 8 GB VRAM. This marks a departure from previous proprietary text-to-image models such as DALL-E and Midjourney, which were accessible only via cloud services."),(0,n.kt)("p",null,"To better understand Stable Diffusion and how it works, there are several visual guides available. Jalammar's blog (",(0,n.kt)("a",{parentName:"p",href:"https://jalammar.github.io/illustrated-stable-diffusion/"},"https://jalammar.github.io/illustrated-stable-diffusion/"),") provides an illustrated guide to the model, while the Stable Diffusion Art website (",(0,n.kt)("a",{parentName:"p",href:"https://stable-diffusion-art.com/how-stable-diffusion-work/"},"https://stable-diffusion-art.com/how-stable-diffusion-work/"),") offers a step-by-step breakdown of the process."),(0,n.kt)("p",null,"In addition, a Colab notebook (",(0,n.kt)("a",{parentName:"p",href:"https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1?usp=sharing"},"https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1?usp=sharing"),") is available to allow users to experiment with and gain a deeper understanding of the Stable Diffusion model."),(0,n.kt)("p",null,"Wikiepedia: ",(0,n.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Stable_Diffusion"},"https://en.wikipedia.org/wiki/Stable_Diffusion"),(0,n.kt)("br",{parentName:"p"}),"\n","source code: ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/justinpinkney/stable-diffusion"},"https://github.com/justinpinkney/stable-diffusion"),(0,n.kt)("br",{parentName:"p"}),"\n","Homepage: ",(0,n.kt)("a",{parentName:"p",href:"https://stability.ai/"},"https://stability.ai/")),(0,n.kt)("h3",{id:"origins-and-research-of-stable-diffusion"},"Origins and Research of Stable Diffusion"),(0,n.kt)("p",null,"Stable Diffusion (SD) is a deep-learning, text-to-image model that was released in 2022. It was developed by the CompVis group at LMU Munich in collaboration with Stability AI, Runway, EleutherAI, and LAION. The model was created through extensive research into deep generative neural networks and the diffusion process."),(0,n.kt)("p",null,"In the original announcement (",(0,n.kt)("a",{parentName:"p",href:"https://stability.ai/blog/stable-diffusion-announcement"},"https://stability.ai/blog/stable-diffusion-announcement"),"), the creators of SD outlined the model's key features and capabilities. These include the ability to generate high-quality images based on text descriptions, as well as the flexibility to be applied to other tasks such as inpainting and image-to-image translation."),(0,n.kt)("p",null,"Stable Diffusion is a latent diffusion model, which is a type of deep generative neural network that uses a process of random noise generation and diffusion to create images. The model is trained on large datasets of images and text descriptions to learn the relationships between the two. This training process involves extensive experimentation and optimization to ensure that the model can accurately generate images based on text prompts."),(0,n.kt)("p",null,"The source code for Stable Diffusion is publicly available on GitHub (",(0,n.kt)("a",{parentName:"p",href:"https://github.com/CompVis/stable-diffusion"},"https://github.com/CompVis/stable-diffusion"),"). This allows researchers and developers to experiment with the model, contribute to its development, and use it for their own projects."),(0,n.kt)("p",null,"Stability AI, the primary sponsor of Stable Diffusion, raised US$101 million in October 2022 to support further research and development of the model. The success of the model has highlighted the potential of deep learning and generative neural networks in the field of computer vision and image generation."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://research.runwayml.com/the-research-origins-of-stable-difussion"},"https://research.runwayml.com/the-research-origins-of-stable-difussion")),(0,n.kt)("h4",{id:"initial-training-data"},"Initial Training Data"),(0,n.kt)("p",null,"LAION-5B - 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution",(0,n.kt)("br",{parentName:"p"}),"\n","Laion-Aesthetics v2 5+"))}H.isMDXComponent=!0},13026:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/sd-fd2a87325289a0c2371f70e915eeb3f4.png"}}]);