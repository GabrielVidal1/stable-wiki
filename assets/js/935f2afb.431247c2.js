"use strict";(self.webpackChunkstable_wiki=self.webpackChunkstable_wiki||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Welcome","href":"/docs/intro","docId":"intro"},{"type":"category","label":"Stable diffusion","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"VAE","href":"/docs/stable-diffusion/VAE","docId":"stable-diffusion/VAE"},{"type":"link","label":"Diffuser","href":"/docs/stable-diffusion/diffuser","docId":"stable-diffusion/diffuser"},{"type":"link","label":"Text encoder","href":"/docs/stable-diffusion/text-encoder","docId":"stable-diffusion/text-encoder"}],"href":"/docs/stable-diffusion/"},{"type":"category","label":"Customization","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fine tuning","href":"/docs/customize/fine-tuning","docId":"customize/fine-tuning"},{"type":"link","label":"Textual inversion","href":"/docs/customize/textual-inversion","docId":"customize/textual-inversion"},{"type":"link","label":"Dreambooth","href":"/docs/customize/dreambooth","docId":"customize/dreambooth"},{"type":"link","label":"Hypernetworks","href":"/docs/customize/hypernetworks","docId":"customize/hypernetworks"},{"type":"link","label":"LoRA","href":"/docs/customize/LoRA","docId":"customize/LoRA"},{"type":"link","label":"ControlNet","href":"/docs/customize/ControlNet","docId":"customize/ControlNet"}],"href":"/docs/category/customization"}]},"docs":{"customize/ControlNet":{"id":"customize/ControlNet","title":"ControlNet","description":"ControlNet is a neural network structure to control diffusion models by adding extra conditions.","sidebar":"tutorialSidebar"},"customize/dreambooth":{"id":"customize/dreambooth","title":"Dreambooth","description":"Dreambooth is a tool that allows you to fine-tune a Stable Diffusion checkpoint based on a single keyword that represents all of your images, for example, \\"mycat.\\" This approach does not require you to caption each individual image, which can save time and effort. To use Dreambooth, you need to prepare at least 20 images in a square format of 512x512 or 768x768 and fine-tune the Stable Diffusion checkpoint on them. This process requires a significant amount of VRAM, typically above 15GB, and will produce a file ranging from 2GB to 5GB. Accumulative stacking is also possible in Dreambooth, which involves consecutive training while maintaining the structure of the models. However, this technique is challenging to execute. Overall, Dreambooth can be a useful tool for fine-tuning a Stable Diffusion checkpoint to a specific set of images using a single keyword.","sidebar":"tutorialSidebar"},"customize/fine-tuning":{"id":"customize/fine-tuning","title":"Fine tuning","description":"To fine-tune a model, you start with a pre-trained checkpoint or diffuser and then continue training it on your own dataset or with your own prompts. This allows you to customize the model to better fit your specific needs. Checkpoints are saved models that can be loaded to continue training or to generate images. Diffusers, on the other hand, are used for guiding the diffusion process during image generation.","sidebar":"tutorialSidebar"},"customize/hypernetworks":{"id":"customize/hypernetworks","title":"Hypernetworks","description":"Hypernetworks are a machine learning technique that allows for the training of a model without altering its weights. This technique involves the use of a separate small network, known as a hypernetwork, to modify the generated images after they have been created. This approach can be useful for fine-tuning generated images without changing the underlying model architecture.","sidebar":"tutorialSidebar"},"customize/LoRA":{"id":"customize/LoRA","title":"LoRA","description":"LORA, or Low-Rank Adaptation, is a technique for training a model to a specific subject or style. LORA is advantageous over Dreambooth in that it only requires 6GB of VRAM to run and produces two small files of 6MB, making it less hardware-intensive. However, it is less flexible than Dreambooth and primarily focuses on faces. LORA can be thought of as injecting a part of a model and teaching it new concepts, making it a powerful tool for fine-tuning generated images without altering the underlying model architecture. One of the primary benefits of LORA is that it has a lower hardware requirement to train, although it can be more complex to train than other techniques. It also does not water down the model in the same way that merging models does.","sidebar":"tutorialSidebar"},"customize/textual-inversion":{"id":"customize/textual-inversion","title":"Textual inversion","description":"Textual Inversion","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Welcome","description":"Welcome to this course on prompt engineering!","sidebar":"tutorialSidebar"},"stable-diffusion/diffuser":{"id":"stable-diffusion/diffuser","title":"Diffuser","description":"The diffuser is a neural network with a U-Net architecture","sidebar":"tutorialSidebar"},"stable-diffusion/README":{"id":"stable-diffusion/README","title":"Stable diffusion","description":"What is Stable Diffusion","sidebar":"tutorialSidebar"},"stable-diffusion/text-encoder":{"id":"stable-diffusion/text-encoder","title":"Text encoder","description":"Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder1. The text encoder is used to turn your prompt into a latent vector","sidebar":"tutorialSidebar"},"stable-diffusion/VAE":{"id":"stable-diffusion/VAE","title":"VAE","description":"The simplest explanation is that it makes an image small then makes it bigger again.","sidebar":"tutorialSidebar"}}}')}}]);